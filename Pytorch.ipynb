{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25293cec-57ea-4cd9-bcfb-504b40ee0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Örnek veri yükleme\n",
    "data = pd.read_csv(\"birlesik_data.csv\")  # haberler.csv dosyanızı uygun şekilde yükleyin\n",
    "\n",
    "# Veri temizleme fonksiyonu\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Fazla boşlukları kaldırma\n",
    "    text = re.sub(r'\\W', ' ', text)  # Alfasayısal olmayan karakterleri kaldırma\n",
    "    text = text.lower()  # Küçük harfe çevirme\n",
    "    return text\n",
    "\n",
    "data['cleaned_text'] = data['content'].apply(clean_text)\n",
    "\n",
    "# Örnek etiketli veri (manuel olarak oluşturun veya mevcut verilerden seçin)\n",
    "labeled_data = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"Küresel ısınma dünya çapında ciddi etkiler yaratıyor...\",\n",
    "        \"Ekonomi bakanı yeni teşvik paketini açıkladı...\",\n",
    "        \"Deniz seviyelerinin yükselmesi kıyı şehirlerini tehdit ediyor...\",\n",
    "        \"Yeni bir güneş enerjisi santrali açıldı...\",\n",
    "        \"Yerel halk, orman yangınlarına karşı önlem alıyor...\",\n",
    "        \"Teknoloji şirketi yeni bir akıllı telefon tanıttı...\",\n",
    "        \"Plastik atıkların okyanuslara zarar verdiği tespit edildi...\",\n",
    "        \"Uluslararası anlaşmalar iklim değişikliğiyle mücadeleyi hedefliyor...\",\n",
    "        \"Spor müsabakalarında büyük heyecan yaşandı...\",\n",
    "        \"Yeni tarım teknikleri sürdürülebilirlik sağlıyor...\",\n",
    "        \"Hava kirliliği insan sağlığını olumsuz etkiliyor...\",\n",
    "        \"Fosil yakıtların kullanımının azaltılması gerektiği belirtiliyor...\",\n",
    "        \"Ormanlar karbon emilimi için kritik öneme sahip...\",\n",
    "        \"Yeni yürüyüş parkuru doğaseverlerin ilgisini çekiyor...\",\n",
    "        \"İklim değişikliği nedeniyle kuraklık artıyor...\",\n",
    "        \"Elektrikli araçların yaygınlaşması karbon ayak izini azaltıyor...\",\n",
    "        \"Yerel yönetimler geri dönüşüm programlarını artırıyor...\",\n",
    "        \"Yapay zeka alanında büyük ilerlemeler kaydedildi...\",\n",
    "        \"Sanat galerisi yeni sergi açtı...\",\n",
    "        \"Geleceğin enerji kaynakları yenilenebilir enerji olacak...\",\n",
    "        \"Şehirde toplu taşıma sistemleri genişletiliyor...\",\n",
    "        \"Küresel biyolojik çeşitlilik tehlikede...\",\n",
    "        \"Çevre dostu binalar enerji tasarrufu sağlıyor...\",\n",
    "        \"Uzay araştırmalarında yeni keşifler yapıldı...\",\n",
    "        \"Geri dönüşüm fabrikası yeni tesis açtı...\",\n",
    "        \"Ulusal parklar biyolojik çeşitliliği korumada önemli rol oynuyor...\",\n",
    "        \"Orman yangınları doğal yaşamı tehdit ediyor...\",\n",
    "        \"Temiz enerji teknolojileri hızla gelişiyor...\",\n",
    "        \"Gıda israfını azaltma kampanyaları başlatıldı...\",\n",
    "        \"Su kaynaklarının korunması için yeni düzenlemeler yapıldı...\"\n",
    "    ],\n",
    "    'label': [\n",
    "        1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
    "        1, 1, 0, 1, 1, 1, 1, 1, 1, 1\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Eğitim ve doğrulama setlerine bölme\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    labeled_data['text'], labeled_data['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF vektörizasyonu\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "train_vectors = vectorizer.fit_transform(train_texts)\n",
    "val_vectors = vectorizer.transform(val_texts)\n",
    "\n",
    "# Verileri Tensor'a dönüştürme\n",
    "train_vectors = torch.tensor(train_vectors.toarray(), dtype=torch.float32)\n",
    "val_vectors = torch.tensor(val_vectors.toarray(), dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "val_labels = torch.tensor(val_labels.values, dtype=torch.long)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(train_vectors, train_labels)\n",
    "val_dataset = TensorDataset(val_vectors, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d12af650-5bcb-4b9b-bc5a-c605dbbccb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 0.1125, Accuracy: 66.67%\n",
      "Epoch: 2, Validation Loss: 0.1105, Accuracy: 66.67%\n",
      "Epoch: 3, Validation Loss: 0.1085, Accuracy: 66.67%\n",
      "Epoch: 4, Validation Loss: 0.1066, Accuracy: 66.67%\n",
      "Epoch: 5, Validation Loss: 0.1049, Accuracy: 66.67%\n",
      "Epoch: 6, Validation Loss: 0.1032, Accuracy: 66.67%\n",
      "Epoch: 7, Validation Loss: 0.1018, Accuracy: 66.67%\n",
      "Epoch: 8, Validation Loss: 0.1007, Accuracy: 66.67%\n",
      "Epoch: 9, Validation Loss: 0.0999, Accuracy: 66.67%\n",
      "Epoch: 10, Validation Loss: 0.0997, Accuracy: 66.67%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Modeli oluşturma\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_size = train_vectors.shape[1]\n",
    "model = SimpleNN(input_size)\n",
    "\n",
    "# Kayıp fonksiyonu ve optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Modeli eğitme\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    print(f'Epoch: {epoch}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d219b483-673e-4cc6-aeeb-45b3221f9037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Etiketlenmemiş veri\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m unlabeled_texts \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Tüm etiketlenmemiş veriyi kullanma\u001b[39;00m\n\u001b[1;32m      3\u001b[0m unlabeled_vectors \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(unlabeled_texts)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Pseudo-labeling\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "unlabeled_texts = data['cleaned_text'].tolist()  \n",
    "unlabeled_vectors = vectorizer.transform(unlabeled_texts).toarray()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "unlabeled_vectors = torch.tensor(unlabeled_vectors, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    pseudo_outputs = model(unlabeled_vectors)\n",
    "    pseudo_labels = pseudo_outputs.argmax(dim=1).numpy()\n",
    "\n",
    "\n",
    "combined_vectors = torch.cat((train_vectors, torch.tensor(unlabeled_vectors, dtype=torch.float32)), dim=0)\n",
    "combined_labels = torch.cat((train_labels, torch.tensor(pseudo_labels, dtype=torch.long)), dim=0)\n",
    "\n",
    "combined_dataset = TensorDataset(combined_vectors, combined_labels)\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for data, target in combined_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    print(f'Epoch: {epoch+1}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
